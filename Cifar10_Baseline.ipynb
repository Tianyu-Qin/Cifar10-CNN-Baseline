{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel was built for cifar10 classification.  \n",
    "\n",
    "You can use this kernel to become familiar with Pytorch and CNN.  \n",
    "\n",
    "You can easily change the dataset and CNNs you use to build another baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "# Preparations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os, sys\n",
    "import copy\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print(\"Using CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 images under train\n",
      "Loaded 10000 images under test\n",
      "Classes: \n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "# Load the data. Calculate the images in each set and print the number of class\n",
    "data_dir = 'cifar10'\n",
    "Train = 'train'\n",
    "Test = 'test'\n",
    "\n",
    "# Transform image data to suitable size with data augmentation\n",
    "data_transforms = {\n",
    "    Train: transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\n",
    "    ]),\n",
    "    Test: transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Specify the datasets\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(\n",
    "        os.path.join(data_dir, x), \n",
    "        transform=data_transforms[x]\n",
    "    )\n",
    "    for x in [Train, Test]\n",
    "    }\n",
    "\n",
    "# Build the dataloader\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x], batch_size=64,\n",
    "        shuffle=True, num_workers=4\n",
    "    )\n",
    "    for x in [Train, Test]\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [Train, Test]}\n",
    "\n",
    "for x in [Train, Test]:\n",
    "    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
    "    \n",
    "print(\"Classes: \")\n",
    "class_names = image_datasets[Train].classes \n",
    "print(image_datasets[Train].classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the accuracy over the k top predictions for the specified values of k\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes and stores the average and current value\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training process and return the accuracy\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (inputs, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda()\n",
    "        input_var = torch.autograd.Variable(inputs.cuda())\n",
    "        target_var = torch.autograd.Variable(target.cuda())\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)\n",
    "        losses.update(loss.data.item(), inputs.size(0))\n",
    "        top1.update(prec1[0][0], inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            curr_lr = optimizer.param_groups[0]['lr']\n",
    "            print('Epoch: [{0}/{1}][{2}/{3}]\\t'\n",
    "                  'LR: {4}\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                   epoch, 200, i, len(train_loader), curr_lr,\n",
    "                   batch_time=batch_time, data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "    print(' * Training Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "    \n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the validation process and return the accuracy\n",
    "def validate(test_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (inputs, target) in enumerate(test_loader):\n",
    "        target = target.cuda()\n",
    "        input_var = torch.autograd.Variable(inputs.cuda(), volatile=True)\n",
    "        target_var = torch.autograd.Variable(target.cuda(), volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)\n",
    "        losses.update(loss.data.item(), inputs.size(0))\n",
    "        top1.update(prec1[0][0], inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                   i, len(test_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Validation Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the training model\n",
    "\n",
    "Resnet18 = models.resnet18(pretrained = True)\n",
    "Resnet18.fc = nn.Linear(512, len(class_names))\n",
    "Resnet18.cuda()\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(Resnet18.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "# Store the training and validation error\n",
    "valError = [0.9]\n",
    "trainError = [0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/200][0/782]\tLR: 0.01\tTime 5.173 (5.173)\tData 3.943 (3.943)\tLoss 2.5439 (2.5439)\tPrec@1 10.938 (10.938)\n",
      "Epoch: [0/200][100/782]\tLR: 0.01\tTime 0.358 (0.408)\tData 0.005 (0.046)\tLoss 1.2828 (1.4058)\tPrec@1 53.125 (50.619)\n",
      "Epoch: [0/200][200/782]\tLR: 0.01\tTime 0.359 (0.385)\tData 0.005 (0.027)\tLoss 0.9224 (1.2397)\tPrec@1 65.625 (56.841)\n",
      "Epoch: [0/200][300/782]\tLR: 0.01\tTime 0.364 (0.378)\tData 0.005 (0.020)\tLoss 1.5147 (1.1451)\tPrec@1 60.938 (60.263)\n",
      "Epoch: [0/200][400/782]\tLR: 0.01\tTime 0.365 (0.375)\tData 0.005 (0.017)\tLoss 0.8399 (1.0796)\tPrec@1 68.750 (62.722)\n",
      "Epoch: [0/200][500/782]\tLR: 0.01\tTime 0.362 (0.373)\tData 0.005 (0.015)\tLoss 0.8157 (1.0278)\tPrec@1 71.875 (64.533)\n",
      "Epoch: [0/200][600/782]\tLR: 0.01\tTime 0.362 (0.371)\tData 0.005 (0.014)\tLoss 1.0824 (0.9850)\tPrec@1 65.625 (66.033)\n",
      "Epoch: [0/200][700/782]\tLR: 0.01\tTime 0.370 (0.371)\tData 0.005 (0.013)\tLoss 0.9652 (0.9559)\tPrec@1 68.750 (67.009)\n",
      " * Training Prec@1 67.664\n",
      "Training Time 289.359\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vogelyu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  del sys.path[0]\n",
      "c:\\users\\vogelyu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/157]\tTime 4.029 (4.029)\tLoss 0.7469 (0.7469)\tPrec@1 73.438 (73.438)\n",
      "Test: [100/157]\tTime 0.123 (0.163)\tLoss 0.8088 (0.7509)\tPrec@1 71.875 (74.072)\n",
      " * Validation Prec@1 73.890\n",
      "Validation Time 23.377\t\n",
      "Epoch: [1/200][0/782]\tLR: 0.01\tTime 4.120 (4.120)\tData 3.968 (3.968)\tLoss 0.5472 (0.5472)\tPrec@1 84.375 (84.375)\n",
      "Epoch: [1/200][100/782]\tLR: 0.01\tTime 0.368 (0.402)\tData 0.005 (0.046)\tLoss 0.6875 (0.7110)\tPrec@1 70.312 (75.557)\n",
      "Epoch: [1/200][200/782]\tLR: 0.01\tTime 0.366 (0.384)\tData 0.006 (0.027)\tLoss 0.7878 (0.7204)\tPrec@1 73.438 (75.008)\n",
      "Epoch: [1/200][300/782]\tLR: 0.01\tTime 0.363 (0.378)\tData 0.005 (0.020)\tLoss 0.7187 (0.7140)\tPrec@1 75.000 (75.114)\n",
      "Epoch: [1/200][400/782]\tLR: 0.01\tTime 0.368 (0.375)\tData 0.005 (0.017)\tLoss 0.5283 (0.7032)\tPrec@1 82.812 (75.736)\n",
      "Epoch: [1/200][500/782]\tLR: 0.01\tTime 0.368 (0.373)\tData 0.005 (0.015)\tLoss 0.6493 (0.7003)\tPrec@1 78.125 (75.805)\n",
      "Epoch: [1/200][600/782]\tLR: 0.01\tTime 0.363 (0.372)\tData 0.005 (0.014)\tLoss 0.9543 (0.6979)\tPrec@1 64.062 (75.863)\n",
      "Epoch: [1/200][700/782]\tLR: 0.01\tTime 0.369 (0.371)\tData 0.005 (0.013)\tLoss 0.7267 (0.6943)\tPrec@1 78.125 (75.996)\n",
      " * Training Prec@1 76.196\n",
      "Training Time 289.979\t\n",
      "Test: [0/157]\tTime 3.998 (3.998)\tLoss 0.6666 (0.6666)\tPrec@1 76.562 (76.562)\n",
      "Test: [100/157]\tTime 0.135 (0.167)\tLoss 0.5603 (0.6645)\tPrec@1 78.125 (77.259)\n",
      " * Validation Prec@1 77.340\n",
      "Validation Time 23.997\t\n",
      "Epoch: [2/200][0/782]\tLR: 0.01\tTime 4.090 (4.090)\tData 3.940 (3.940)\tLoss 0.7389 (0.7389)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [2/200][100/782]\tLR: 0.01\tTime 0.367 (0.402)\tData 0.005 (0.046)\tLoss 0.6852 (0.6367)\tPrec@1 75.000 (78.048)\n",
      "Epoch: [2/200][200/782]\tLR: 0.01\tTime 0.365 (0.385)\tData 0.006 (0.027)\tLoss 0.6288 (0.6249)\tPrec@1 78.125 (78.762)\n",
      "Epoch: [2/200][300/782]\tLR: 0.01\tTime 0.366 (0.379)\tData 0.005 (0.020)\tLoss 0.5410 (0.6168)\tPrec@1 85.938 (79.028)\n",
      "Epoch: [2/200][400/782]\tLR: 0.01\tTime 0.363 (0.376)\tData 0.005 (0.017)\tLoss 0.5811 (0.6160)\tPrec@1 81.250 (78.998)\n",
      "Epoch: [2/200][500/782]\tLR: 0.01\tTime 0.365 (0.374)\tData 0.006 (0.015)\tLoss 0.7785 (0.6161)\tPrec@1 68.750 (78.774)\n",
      "Epoch: [2/200][600/782]\tLR: 0.01\tTime 0.369 (0.373)\tData 0.005 (0.014)\tLoss 0.8072 (0.6149)\tPrec@1 71.875 (78.767)\n",
      "Epoch: [2/200][700/782]\tLR: 0.01\tTime 0.366 (0.372)\tData 0.005 (0.013)\tLoss 0.5465 (0.6151)\tPrec@1 81.250 (78.807)\n",
      " * Training Prec@1 78.850\n",
      "Training Time 290.468\t\n",
      "Test: [0/157]\tTime 4.013 (4.013)\tLoss 0.6300 (0.6300)\tPrec@1 70.312 (70.312)\n",
      "Test: [100/157]\tTime 0.122 (0.168)\tLoss 0.3623 (0.6200)\tPrec@1 90.625 (78.775)\n",
      " * Validation Prec@1 78.890\n",
      "Validation Time 24.182\t\n",
      "Epoch: [3/200][0/782]\tLR: 0.01\tTime 4.099 (4.099)\tData 3.947 (3.947)\tLoss 0.6893 (0.6893)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [3/200][100/782]\tLR: 0.01\tTime 0.366 (0.404)\tData 0.005 (0.046)\tLoss 0.5836 (0.6278)\tPrec@1 82.812 (78.450)\n",
      "Epoch: [3/200][200/782]\tLR: 0.01\tTime 0.370 (0.386)\tData 0.005 (0.027)\tLoss 0.5430 (0.5913)\tPrec@1 79.688 (79.851)\n",
      "Epoch: [3/200][300/782]\tLR: 0.01\tTime 0.366 (0.380)\tData 0.005 (0.020)\tLoss 0.5406 (0.5709)\tPrec@1 76.562 (80.264)\n",
      "Epoch: [3/200][400/782]\tLR: 0.01\tTime 0.366 (0.376)\tData 0.005 (0.017)\tLoss 0.3479 (0.5685)\tPrec@1 90.625 (80.365)\n",
      "Epoch: [3/200][500/782]\tLR: 0.01\tTime 0.370 (0.375)\tData 0.005 (0.015)\tLoss 0.4904 (0.5674)\tPrec@1 84.375 (80.333)\n",
      "Epoch: [3/200][600/782]\tLR: 0.01\tTime 0.370 (0.373)\tData 0.005 (0.014)\tLoss 0.6464 (0.5654)\tPrec@1 76.562 (80.345)\n",
      "Epoch: [3/200][700/782]\tLR: 0.01\tTime 0.367 (0.373)\tData 0.005 (0.013)\tLoss 0.4704 (0.5641)\tPrec@1 85.938 (80.425)\n",
      " * Training Prec@1 80.568\n",
      "Training Time 290.749\t\n",
      "Test: [0/157]\tTime 4.022 (4.022)\tLoss 0.6584 (0.6584)\tPrec@1 78.125 (78.125)\n",
      "Test: [100/157]\tTime 0.127 (0.168)\tLoss 0.6393 (0.5910)\tPrec@1 75.000 (79.425)\n",
      " * Validation Prec@1 79.860\n",
      "Validation Time 24.289\t\n",
      "Epoch: [4/200][0/782]\tLR: 0.01\tTime 4.107 (4.107)\tData 3.954 (3.954)\tLoss 0.5195 (0.5195)\tPrec@1 84.375 (84.375)\n",
      "Epoch: [4/200][100/782]\tLR: 0.01\tTime 0.367 (0.403)\tData 0.005 (0.046)\tLoss 0.5825 (0.5333)\tPrec@1 85.938 (81.451)\n",
      "Epoch: [4/200][200/782]\tLR: 0.01\tTime 0.365 (0.385)\tData 0.005 (0.027)\tLoss 0.4812 (0.5278)\tPrec@1 81.250 (81.678)\n",
      "Epoch: [4/200][300/782]\tLR: 0.01\tTime 0.363 (0.379)\tData 0.005 (0.020)\tLoss 0.5316 (0.5218)\tPrec@1 78.125 (81.946)\n",
      "Epoch: [4/200][400/782]\tLR: 0.01\tTime 0.365 (0.376)\tData 0.005 (0.017)\tLoss 0.4940 (0.5209)\tPrec@1 82.812 (81.955)\n",
      "Epoch: [4/200][500/782]\tLR: 0.01\tTime 0.366 (0.374)\tData 0.005 (0.015)\tLoss 0.6065 (0.5217)\tPrec@1 81.250 (81.952)\n",
      "Epoch: [4/200][600/782]\tLR: 0.01\tTime 0.368 (0.373)\tData 0.005 (0.014)\tLoss 0.7351 (0.5247)\tPrec@1 75.000 (81.838)\n",
      "Epoch: [4/200][700/782]\tLR: 0.01\tTime 0.366 (0.372)\tData 0.005 (0.013)\tLoss 0.6176 (0.5253)\tPrec@1 82.812 (81.890)\n",
      " * Training Prec@1 82.008\n",
      "Training Time 290.779\t\n",
      "Test: [0/157]\tTime 4.091 (4.091)\tLoss 0.6500 (0.6500)\tPrec@1 76.562 (76.562)\n",
      "Test: [100/157]\tTime 0.145 (0.185)\tLoss 0.5073 (0.5660)\tPrec@1 82.812 (81.049)\n",
      " * Validation Prec@1 81.050\n",
      "Validation Time 26.777\t\n",
      "Epoch: [5/200][0/782]\tLR: 0.01\tTime 4.130 (4.130)\tData 3.982 (3.982)\tLoss 0.8441 (0.8441)\tPrec@1 76.562 (76.562)\n",
      "Epoch: [5/200][100/782]\tLR: 0.01\tTime 0.371 (0.403)\tData 0.005 (0.046)\tLoss 0.2858 (0.4820)\tPrec@1 90.625 (83.493)\n",
      "Epoch: [5/200][200/782]\tLR: 0.01\tTime 0.365 (0.385)\tData 0.005 (0.027)\tLoss 0.4489 (0.4873)\tPrec@1 84.375 (83.139)\n",
      "Epoch: [5/200][300/782]\tLR: 0.01\tTime 0.368 (0.379)\tData 0.005 (0.020)\tLoss 0.9643 (0.4970)\tPrec@1 64.062 (82.761)\n",
      "Epoch: [5/200][400/782]\tLR: 0.01\tTime 0.366 (0.376)\tData 0.005 (0.017)\tLoss 0.5142 (0.5007)\tPrec@1 89.062 (82.684)\n",
      "Epoch: [5/200][500/782]\tLR: 0.01\tTime 0.369 (0.375)\tData 0.005 (0.015)\tLoss 0.4218 (0.5033)\tPrec@1 84.375 (82.666)\n",
      "Epoch: [5/200][600/782]\tLR: 0.01\tTime 0.365 (0.374)\tData 0.005 (0.014)\tLoss 0.5125 (0.4993)\tPrec@1 81.250 (82.766)\n",
      "Epoch: [5/200][700/782]\tLR: 0.01\tTime 0.366 (0.373)\tData 0.005 (0.013)\tLoss 0.4166 (0.5006)\tPrec@1 82.812 (82.648)\n",
      " * Training Prec@1 82.694\n",
      "Training Time 291.011\t\n",
      "Test: [0/157]\tTime 4.086 (4.086)\tLoss 0.4909 (0.4909)\tPrec@1 82.812 (82.812)\n",
      "Test: [100/157]\tTime 0.130 (0.169)\tLoss 0.2658 (0.5289)\tPrec@1 87.500 (81.544)\n",
      " * Validation Prec@1 81.750\n",
      "Validation Time 24.246\t\n",
      "Epoch: [6/200][0/782]\tLR: 0.01\tTime 4.149 (4.149)\tData 4.000 (4.000)\tLoss 0.3497 (0.3497)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [6/200][100/782]\tLR: 0.01\tTime 0.334 (0.375)\tData 0.005 (0.047)\tLoss 0.1676 (0.4876)\tPrec@1 98.438 (83.137)\n",
      "Epoch: [6/200][200/782]\tLR: 0.01\tTime 0.337 (0.356)\tData 0.005 (0.027)\tLoss 0.4967 (0.4821)\tPrec@1 81.250 (83.302)\n",
      "Epoch: [6/200][300/782]\tLR: 0.01\tTime 0.337 (0.350)\tData 0.005 (0.021)\tLoss 0.2073 (0.4799)\tPrec@1 92.188 (83.259)\n",
      "Epoch: [6/200][400/782]\tLR: 0.01\tTime 0.338 (0.346)\tData 0.006 (0.017)\tLoss 0.4524 (0.4809)\tPrec@1 87.500 (83.358)\n",
      "Epoch: [6/200][500/782]\tLR: 0.01\tTime 0.339 (0.344)\tData 0.005 (0.015)\tLoss 0.6476 (0.4815)\tPrec@1 76.562 (83.258)\n",
      "Epoch: [6/200][600/782]\tLR: 0.01\tTime 0.333 (0.343)\tData 0.005 (0.014)\tLoss 0.3611 (0.4814)\tPrec@1 85.938 (83.205)\n",
      "Epoch: [6/200][700/782]\tLR: 0.01\tTime 0.337 (0.342)\tData 0.005 (0.013)\tLoss 0.3759 (0.4835)\tPrec@1 89.062 (83.216)\n",
      " * Training Prec@1 83.186\n",
      "Training Time 267.482\t\n",
      "Test: [0/157]\tTime 4.058 (4.058)\tLoss 0.5570 (0.5570)\tPrec@1 81.250 (81.250)\n",
      "Test: [100/157]\tTime 0.127 (0.169)\tLoss 0.5134 (0.5223)\tPrec@1 79.688 (81.730)\n",
      " * Validation Prec@1 81.870\n",
      "Validation Time 24.312\t\n",
      "Epoch: [7/200][0/782]\tLR: 0.01\tTime 4.129 (4.129)\tData 3.978 (3.978)\tLoss 0.4138 (0.4138)\tPrec@1 89.062 (89.062)\n",
      "Epoch: [7/200][100/782]\tLR: 0.01\tTime 0.340 (0.374)\tData 0.005 (0.047)\tLoss 0.2828 (0.4479)\tPrec@1 93.750 (83.988)\n",
      "Epoch: [7/200][200/782]\tLR: 0.01\tTime 0.334 (0.355)\tData 0.005 (0.027)\tLoss 0.2917 (0.4541)\tPrec@1 90.625 (84.072)\n",
      "Epoch: [7/200][300/782]\tLR: 0.01\tTime 0.338 (0.349)\tData 0.005 (0.021)\tLoss 0.4590 (0.4572)\tPrec@1 82.812 (83.892)\n",
      "Epoch: [7/200][400/782]\tLR: 0.01\tTime 0.339 (0.346)\tData 0.004 (0.017)\tLoss 0.5783 (0.4621)\tPrec@1 78.125 (83.794)\n",
      "Epoch: [7/200][500/782]\tLR: 0.01\tTime 0.336 (0.344)\tData 0.005 (0.015)\tLoss 0.6537 (0.4604)\tPrec@1 75.000 (83.901)\n",
      "Epoch: [7/200][600/782]\tLR: 0.01\tTime 0.338 (0.343)\tData 0.005 (0.014)\tLoss 0.5246 (0.4584)\tPrec@1 84.375 (84.011)\n",
      "Epoch: [7/200][700/782]\tLR: 0.01\tTime 0.337 (0.342)\tData 0.006 (0.013)\tLoss 0.3922 (0.4588)\tPrec@1 84.375 (83.994)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Training Prec@1 84.088\n",
      "Training Time 267.448\t\n",
      "Test: [0/157]\tTime 4.020 (4.020)\tLoss 0.4223 (0.4223)\tPrec@1 84.375 (84.375)\n",
      "Test: [100/157]\tTime 0.132 (0.168)\tLoss 0.7632 (0.5019)\tPrec@1 71.875 (82.766)\n",
      " * Validation Prec@1 83.060\n",
      "Validation Time 24.229\t\n",
      "Epoch: [8/200][0/782]\tLR: 0.01\tTime 4.106 (4.106)\tData 3.954 (3.954)\tLoss 0.5210 (0.5210)\tPrec@1 84.375 (84.375)\n",
      "Epoch: [8/200][100/782]\tLR: 0.01\tTime 0.333 (0.374)\tData 0.005 (0.046)\tLoss 0.5269 (0.4386)\tPrec@1 81.250 (84.684)\n",
      "Epoch: [8/200][200/782]\tLR: 0.01\tTime 0.337 (0.356)\tData 0.006 (0.027)\tLoss 0.4914 (0.4483)\tPrec@1 84.375 (84.072)\n",
      "Epoch: [8/200][300/782]\tLR: 0.01\tTime 0.336 (0.349)\tData 0.005 (0.020)\tLoss 0.2915 (0.4553)\tPrec@1 90.625 (84.012)\n",
      "Epoch: [8/200][400/782]\tLR: 0.01\tTime 0.336 (0.346)\tData 0.005 (0.017)\tLoss 0.3272 (0.4551)\tPrec@1 87.500 (84.083)\n",
      "Epoch: [8/200][500/782]\tLR: 0.01\tTime 0.340 (0.345)\tData 0.005 (0.015)\tLoss 0.3356 (0.4534)\tPrec@1 84.375 (84.191)\n",
      "Epoch: [8/200][600/782]\tLR: 0.01\tTime 0.340 (0.344)\tData 0.005 (0.014)\tLoss 0.3537 (0.4482)\tPrec@1 90.625 (84.419)\n",
      "Epoch: [8/200][700/782]\tLR: 0.01\tTime 0.351 (0.344)\tData 0.006 (0.013)\tLoss 0.4273 (0.4498)\tPrec@1 84.375 (84.375)\n",
      " * Training Prec@1 84.416\n",
      "Training Time 269.576\t\n",
      "Test: [0/157]\tTime 4.143 (4.143)\tLoss 0.4148 (0.4148)\tPrec@1 84.375 (84.375)\n",
      "Test: [100/157]\tTime 0.130 (0.173)\tLoss 0.4803 (0.4861)\tPrec@1 84.375 (83.292)\n",
      " * Validation Prec@1 83.050\n",
      "Validation Time 24.732\t\n",
      "Epoch: [9/200][0/782]\tLR: 0.01\tTime 4.097 (4.097)\tData 3.945 (3.945)\tLoss 0.3714 (0.3714)\tPrec@1 84.375 (84.375)\n",
      "Epoch: [9/200][100/782]\tLR: 0.01\tTime 0.364 (0.402)\tData 0.005 (0.046)\tLoss 0.3644 (0.4285)\tPrec@1 85.938 (85.303)\n",
      "Epoch: [9/200][200/782]\tLR: 0.01\tTime 0.366 (0.374)\tData 0.006 (0.027)\tLoss 0.3612 (0.4208)\tPrec@1 87.500 (85.393)\n",
      "Epoch: [9/200][300/782]\tLR: 0.01\tTime 0.365 (0.371)\tData 0.005 (0.020)\tLoss 0.3770 (0.4273)\tPrec@1 89.062 (85.252)\n",
      "Epoch: [9/200][400/782]\tLR: 0.01\tTime 0.369 (0.371)\tData 0.006 (0.017)\tLoss 0.4205 (0.4295)\tPrec@1 82.812 (85.115)\n",
      "Epoch: [9/200][500/782]\tLR: 0.01\tTime 0.366 (0.370)\tData 0.005 (0.015)\tLoss 0.3556 (0.4322)\tPrec@1 89.062 (85.039)\n",
      "Epoch: [9/200][600/782]\tLR: 0.01\tTime 0.369 (0.370)\tData 0.005 (0.014)\tLoss 0.3239 (0.4317)\tPrec@1 89.062 (85.090)\n",
      "Epoch: [9/200][700/782]\tLR: 0.01\tTime 0.366 (0.370)\tData 0.005 (0.013)\tLoss 0.3626 (0.4332)\tPrec@1 89.062 (85.046)\n",
      " * Training Prec@1 84.978\n",
      "Training Time 288.700\t\n",
      "Test: [0/157]\tTime 4.000 (4.000)\tLoss 0.6107 (0.6107)\tPrec@1 73.438 (73.438)\n",
      "Test: [100/157]\tTime 0.129 (0.169)\tLoss 0.3880 (0.4762)\tPrec@1 90.625 (83.864)\n",
      " * Validation Prec@1 83.770\n",
      "Validation Time 24.302\t\n",
      "Epoch: [10/200][0/782]\tLR: 0.001\tTime 4.088 (4.088)\tData 3.930 (3.930)\tLoss 0.3981 (0.3981)\tPrec@1 92.188 (92.188)\n",
      "Epoch: [10/200][100/782]\tLR: 0.001\tTime 0.368 (0.404)\tData 0.005 (0.046)\tLoss 0.2258 (0.3779)\tPrec@1 87.500 (86.711)\n",
      "Epoch: [10/200][200/782]\tLR: 0.001\tTime 0.368 (0.388)\tData 0.006 (0.027)\tLoss 0.3739 (0.3738)\tPrec@1 85.938 (86.847)\n",
      "Epoch: [10/200][300/782]\tLR: 0.001\tTime 0.387 (0.382)\tData 0.005 (0.020)\tLoss 0.2994 (0.3679)\tPrec@1 89.062 (87.386)\n",
      "Epoch: [10/200][400/782]\tLR: 0.001\tTime 0.366 (0.380)\tData 0.004 (0.017)\tLoss 0.1556 (0.3623)\tPrec@1 95.312 (87.644)\n",
      "Epoch: [10/200][500/782]\tLR: 0.001\tTime 0.336 (0.376)\tData 0.005 (0.015)\tLoss 0.3575 (0.3558)\tPrec@1 89.062 (87.737)\n",
      "Epoch: [10/200][600/782]\tLR: 0.001\tTime 0.373 (0.374)\tData 0.006 (0.014)\tLoss 0.2160 (0.3510)\tPrec@1 95.312 (87.955)\n",
      "Epoch: [10/200][700/782]\tLR: 0.001\tTime 0.371 (0.375)\tData 0.005 (0.013)\tLoss 0.3964 (0.3498)\tPrec@1 85.938 (87.966)\n",
      " * Training Prec@1 87.948\n",
      "Training Time 292.980\t\n",
      "Test: [0/157]\tTime 4.063 (4.063)\tLoss 0.4499 (0.4499)\tPrec@1 84.375 (84.375)\n",
      "Test: [100/157]\tTime 0.129 (0.169)\tLoss 0.3158 (0.4210)\tPrec@1 90.625 (85.334)\n",
      " * Validation Prec@1 85.680\n",
      "Validation Time 24.325\t\n",
      "Epoch: [11/200][0/782]\tLR: 0.001\tTime 4.126 (4.126)\tData 3.972 (3.972)\tLoss 0.3212 (0.3212)\tPrec@1 85.938 (85.938)\n",
      "Epoch: [11/200][100/782]\tLR: 0.001\tTime 0.331 (0.375)\tData 0.007 (0.047)\tLoss 0.4556 (0.3143)\tPrec@1 79.688 (89.032)\n",
      "Epoch: [11/200][200/782]\tLR: 0.001\tTime 0.345 (0.357)\tData 0.006 (0.027)\tLoss 0.3868 (0.3259)\tPrec@1 87.500 (88.790)\n",
      "Epoch: [11/200][300/782]\tLR: 0.001\tTime 0.340 (0.351)\tData 0.005 (0.021)\tLoss 0.2443 (0.3220)\tPrec@1 92.188 (88.922)\n",
      "Epoch: [11/200][400/782]\tLR: 0.001\tTime 0.345 (0.349)\tData 0.006 (0.017)\tLoss 0.4057 (0.3227)\tPrec@1 90.625 (88.903)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-72faf6f155ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# train for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mpredTrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mResnet18\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtrainError\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpredTrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-348d068561f1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdata_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0minput_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mtarget_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model and evaluate the model after every epoch\n",
    "for epoch in range(30):\n",
    "    if epoch == 10:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.1\n",
    "\n",
    "    # train for one epoch\n",
    "    start = time.time()\n",
    "    predTrain = train(dataloaders[Train], Resnet18, criterion, optimizer, epoch)\n",
    "    trainError.append(1 - predTrain.item()/100)\n",
    "    end = time.time()\n",
    "    print('Training Time {:.3f}\\t'.format(end-start))\n",
    "\n",
    "    # evaluate on validation set\n",
    "    start = time.time()\n",
    "    predVal = validate(dataloaders[Test], Resnet18, criterion)\n",
    "    valError.append(1 - predVal.item()/100)\n",
    "    end = time.time()\n",
    "    print('Validation Time {:.3f}\\t'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
